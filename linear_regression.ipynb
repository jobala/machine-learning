{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression is a simple approach for supervised learning. We are going to use\n",
    "linear regression to predict the price of a house given it's size and the number\n",
    "of bedrooms it has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   area  price\n0     0      0\n1     1      1\n2     2      2\n3     3      3\n4     4      4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>area</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "df = pd.read_csv('houses.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The above dataset will be our training set. **size** is the input variable. Input variables also referred to as\n",
    "features, predictors or independent variables. **price** is our output or target\n",
    "variable.\n",
    "\n",
    "The essence of machine learning is to find a model that **maps** input variables to target variables.\n",
    "Let $X$ denote the input variable space and $Y$ denote the output variable spaces, then the point of\n",
    "machine learning is to find a function $h:X \\mapsto Y$ such that $h(x)$ is a good predictor\n",
    "for the corresponding value of y. This function $h$ is called a **hypothesis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plotting our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c99aba3dbc24104a9f21fa1310993ae"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "plt.scatter(df.area, df.price)\n",
    "plt.title('House Prices')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this case, we can approximate $h$ as a linear function which means that it is a\n",
    "straight line when plotted.\n",
    "\n",
    "For purposes of simplicity let's define $h(x)$  as $h(x) = \\theta x_1$\n",
    "\n",
    "$\\theta$ parameterizes $h$. The task now is to find $\\theta$ such that we get a prediction value\n",
    "closest to the real price value.\n",
    "\n",
    "We can express the hypothesis function in Python as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def hypothesis(theta, x):\n",
    "    return theta * x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cost Function\n",
    "The **cost function** measures how close our prediction is to the real value. It is defined as\n",
    "\n",
    "$J(\\theta)=\\frac{1}{2m}\\sum(h(x_i)-y_i)^2$ where $h(x_i)$ is the prediction for the ith entry in our training set.\n",
    "\n",
    "Writing the cost function in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cost_function(theta, training_data):\n",
    "    result = 0\n",
    "    for index, training_set in training_data.iterrows():\n",
    "        predicted = hypothesis(theta, training_set['area'])\n",
    "        square_diff = (predicted - training_set['price']) ** 2\n",
    "        result = square_diff / (2 * len(training_data.index))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this case a $\\theta$ value of 1, means that our prediction is exactly the same as the real\n",
    "value, therefore, the mean squared difference between our predicted and real values should be 0.\n",
    "In the snippet below, we see that the further we move from 1, the less accurate our prediction becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "theta=1: result= 0.0\ntheta=1.5: result= 0.4\ntheta=2: result= 1.6\n"
    }
   ],
   "source": [
    "print('theta=1: result=', cost_function(1, df))\n",
    "print('theta=1.5: result=', cost_function(1.5, df))\n",
    "print('theta=2: result=', cost_function(2, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualising Hypothesis and Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbf5e029261d4fd58f17167edafa17a3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(FloatSlider(value=1.0, description='theta', max=3.0, min=-1.0), Output()), _dom_classes=…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "235b924cc6f44f4ba2ca8b1d7427927c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<function __main__.update(theta=1.0)>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 5))\n",
    "\n",
    "def plot_prediction(theta):\n",
    "    x1 = np.linspace(0.0, 5.0)\n",
    "    y1 = np.linspace(0.0, 5.0)\n",
    "\n",
    "    axes[0].set_ylim(0, 5.0)\n",
    "\n",
    "    axes[0].set_title('House Prices')\n",
    "    axes[0].set_xlabel('Area(feet$^2$)')\n",
    "    axes[0].set_ylabel('Price')\n",
    "\n",
    "    axes[0].scatter(df.area, df.price)\n",
    "    axes[0].plot(df.area, theta*df.price, color='red', label='prediction')\n",
    "    axes[0].legend(loc='upper left')\n",
    "\n",
    "def plot_cost(theta):\n",
    "    x2 = np.linspace(-0.5, 2.5)\n",
    "    y2 = [cost_function(x, training_data=df) for x in x2]\n",
    "\n",
    "    axes[1].set_ylim(0, 3.5)\n",
    "    axes[1].set_xlim(-0.5, 2.5)\n",
    "\n",
    "    axes[1].set_title('Cost Function')\n",
    "    axes[1].set_xlabel(r'$\\theta$')\n",
    "    axes[1].set_ylabel(r'$J(\\theta)$')\n",
    "\n",
    "    axes[1].scatter(theta, cost_function(theta, df), label='cost')\n",
    "    axes[1].legend(loc='upper left')\n",
    "    axes[1].plot(x2, y2)\n",
    "    \n",
    "plot_prediction(theta=1.0)\n",
    "plot_cost(theta=1.0)\n",
    "\n",
    "def update(theta = 1.0):\n",
    "    axes[0].clear()\n",
    "    axes[1].clear()\n",
    "\n",
    "    plot_prediction(theta)\n",
    "    plot_cost(theta)\n",
    "\n",
    " \n",
    "interact(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the diagram above, we can see how the accuracy of our prediction changes with respect to a change in the cost $J(\\theta)$. Our prediction is equal to the real values when cost 1.\n",
    "\n",
    "## Parameter Learning\n",
    "We have seen by inspection how the parameter $\\theta$ affects our prediction. We also observed that the closer $\\theta$ is to the cost function's global optima the better our prediction.  What we are going to do next is implement **gradient descent**. **Gradient descent** learns the best value for $\\theta$.\n",
    "\n",
    "\n",
    "#### Gradient Descent Intuition\n",
    "1. Start with a guess for $\\theta$ say $\\theta$ = 2\n",
    "2. Find the gradient at $\\theta$ = 2\n",
    "3. Descend the cost function by a value $\\alpha \\times gradient$, $\\alpha$ is called the learning rate\n",
    "4. Repeat until the solution converges. \n",
    "\n",
    "#### Gradient Descent\n",
    "Gradient Descent helps us find the value of $\\theta$ that will give us the best prediction. It works by starting with a guess value say $\\theta = 2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97d0b993f3d149dcb3ec4c97bf62ca79"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def cost(theta):\n",
    "    plt.figure()\n",
    "\n",
    "    x2 = np.linspace(-0.5, 2.5)\n",
    "    y2 = [cost_function(x, training_data=df) for x in x2]\n",
    "\n",
    "    plt.ylim(0, 3.5)\n",
    "    plt.xlim(-0.5, 2.5)\n",
    "\n",
    "    plt.title('Cost Function')\n",
    "    plt.xlabel(r'$\\theta$')\n",
    "    plt.ylabel(r'$J(\\theta)$')\n",
    "\n",
    "    plt.scatter(theta, cost_function(theta, df), label='cost')\n",
    "    plt.plot(x2, y2)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "cost(theta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the current theta position we need to go down towards the global optima. We can do that by\n",
    "1. Getting the gradient at the current $\\theta$ value\n",
    "2. Substracting the gradient from the $\\theta$ to get the new $\\theta$ value\n",
    "\n",
    "Let's indulge in some math\n",
    "\n",
    "The cost function is defined as\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m}\\sum(h(x_i) - y_i)^2$\n",
    "\n",
    "and our prediction function is defined as\n",
    "\n",
    "$h(x_i) = \\theta x_i$\n",
    "\n",
    "Note that the cost function uses the prediction function so we can replace the prediction function in the cost function with the expression $\\theta x_i$. Rewriting the cost function.\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m}\\sum(\\theta x_i - y_i)^2$\n",
    "\n",
    "At $\\theta$ = 2,\n",
    "\n",
    "$J(2) = \\frac{1}{2m}\\sum(2x_i - y_i)^2$, where m is the number of entries in our training data and i is the ith training set.\n",
    "\n",
    "For purpose of demonstration, let's assume we have only one training set, so we won't need to sum them together and we can do away with i. This simplifies our equation to\n",
    "\n",
    "$J(2) = \\frac{1}{2}(2x - y)^2$\n",
    "\n",
    "We can find the gradient at $\\theta = 2$ by applying the **chain rule**\n",
    "\n",
    "$\\frac{d}{d\\theta}J(2) = \\frac{d}{d\\theta}\\frac{1}{2}(2x - y)^2$\n",
    "            \n",
    "$= 2 \\times \\frac{1}{2}(2x - y)x$ \n",
    "\n",
    "$= (2x - y)x$, where x is the house area and y the house price\n",
    "\n",
    "If our training set is (1.5, 1.5), house area of size 3 and price of 3 dollars, then  our new theta will be \n",
    "\n",
    "$\\theta = 2 - (2 \\times 1.5 - 1.5)1.5$\n",
    "\n",
    "$\\theta = 0.25$\n",
    "\n",
    "\n",
    "Plotting our \\theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ff89558c8484e079df6d21f5945627b"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "cost(theta=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the hell just happened? We passed our destination -- global optima. How do we fix this? We'll introduce a learning rate $\\alpha$ that we'll multiply the gradient with to make sure we are taking small steps towards the optima. If you use a big learning rate you might never get to the global optima, if you choose a small learning rate you'll spend 40 years in the wilderness before you reach the promised land -- global optima.\n",
    "\n",
    "Rewriting our equation with a learning rate\n",
    "\n",
    "$\\theta = 2 - \\alpha (2 \\times 1.5 - 1.5)1.5$\n",
    "\n",
    "Let's set $\\alpha$ to a reasonal value say 0.25\n",
    "\n",
    "$\\alpha = 0.25$\n",
    "\n",
    "$\\theta = 2 - 0.25 (2 \\times 1.5 - 1.5)1.5$\n",
    "\n",
    "Then our new $\\theta$ will be\n",
    "\n",
    "$\\theta = 1.437$\n",
    "\n",
    "Plotting our theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01833473f97548acaa984371d093496d"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "cost(theta=1.437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we are moving in the right direction and closer to the global optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalizing Gradient Descent\n",
    "\n",
    "What we saw was a particular solution for $\\theta = 2$. To be able to apply this for any $\\theta$ we need to generalize the gradient descent algorithm. For those who are mathematically inclined they can go through the equations below.\n",
    "\n",
    "$\\theta = \\theta - \\alpha\\frac{d}{d\\theta} J(\\theta)$\n",
    "\n",
    "Since $J(\\theta)=\\frac{1}{2m}\\sum(h(x_i)-y_i)$\n",
    "\n",
    "Then $\\theta = \\theta - \\alpha\\frac{d}{d\\theta} \\frac{1}{2m}\\sum(h(x_i)-y_i)^2$\n",
    "\n",
    "Using the chain rule \n",
    "\n",
    "$\\frac{d}{d\\theta} \\frac{1}{2m}\\sum(h(x_i)-y_i)^2 = \\frac{2}{2m}\\sum(h(x_i)-y_i) \\times \\frac{d}{d\\theta} h(x)$\n",
    "\n",
    "And since $h(x) = \\theta x$ it's derivative with respect to $\\theta$ will be $x$\n",
    "\n",
    "This reduces to\n",
    "\n",
    "$\\theta = \\theta - \\alpha \\frac{1}{m}\\sum(h(x_i) - y_i)x_i$\n",
    "\n",
    "We can express the above equation in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(theta, learning_rate=0.15):\n",
    "    total = 0\n",
    "    m = len(df.index)\n",
    "\n",
    "    for i, training_set in df.iterrows():\n",
    "        prediction = hypothesis(theta, training_set['area'])\n",
    "        error = prediction - training_set['price']\n",
    "        total += (error * training_set['area']) \n",
    "\n",
    "    new_theta = theta - ((learning_rate / m) * total)\n",
    "    return new_theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking gradient descent for a spin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.2000000000000002\n1.02\n1.002\n1.0002\n1.00002\n1.000002\n1.0000002000000001\n1.00000002\n1.000000002\n1.0000000002\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.0000000002"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "\n",
    "\n",
    "def learn():\n",
    "    theta = 3\n",
    "    count = 0\n",
    "\n",
    "    while (count < 10):\n",
    "        theta = gradient_descent(theta)\n",
    "        count += 1\n",
    "        print(theta)\n",
    "    return theta\n",
    "\n",
    "learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As we can see, $\\theta$ gets closer and closer to the desired value 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('experiments': pipenv)",
   "language": "python",
   "name": "python_defaultSpec_1599138084794"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}